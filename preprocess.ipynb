{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turkish Diacritisation | YZV 405E NLP Term Project\n",
    "\n",
    "Author: Bora Boyacıoğlu\n",
    "\n",
    "Student ID: 150200310\n",
    "\n",
    "## Step 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import DiacritizationDataset\n",
    "from utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_example(texts: list[str]):\n",
    "    print('\\033[1mTrain Data Example:\\033[0m')\n",
    "    print('Undiacritized:', texts[0])\n",
    "    print('Diacritized:', texts[1])\n",
    "\n",
    "    print('\\n\\033[1mTest Data Example:\\033[0m')\n",
    "    print('Undiacritized:', texts[2])\n",
    "    print('Diacritized:', texts[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Datasets\n",
    "\n",
    "We have two datasets: `train` and `test`. We will use the `train` dataset to train our model and the `test` dataset to evaluate the model. Firstly, open these datasets using the defined Dataset classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset.\n",
    "train_data = DiacritizationDataset('data/train.csv', type='train')\n",
    "\n",
    "# Test dataset.\n",
    "test_data = DiacritizationDataset('data/test.csv', type='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 57839\t(Train)\n",
      "        1176\t(Test)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length: {len(train_data)}\\t(Train)\\n\"\n",
    "      f\"        {len(test_data)}\\t(Test)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I divided some very long sentences. Also, the $15480^{th}$ sentence in the `train` dataset is a sequence of lines, which created a massive sentence. I divided it line by line. These adjustments made the length a bit different than the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTrain Data Example:\u001b[0m\n",
      "Undiacritized: sinif  havuz ve acik deniz calismalariyla  tum dunyada gecerli  basarili bir standart olusturmustur . \n",
      "Diacritized: sınıf  havuz ve açık deniz çalışmalarıyla  tüm dünyada geçerli  başarılı bir standart oluşturmuştur . \n",
      "\n",
      "\u001b[1mTest Data Example:\u001b[0m\n",
      "Undiacritized:  tr ekonomi ve politika haberleri turkiye nin en cesur gazetesi radikal de uye ol\n",
      "Diacritized: None\n"
     ]
    }
   ],
   "source": [
    "print_example([train_data.get(0, 'und'), train_data.get(0, 'd'), test_data.get(0, 'und'), test_data.get(0, 'd')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the text by converting it to lowercase and removing any special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing text 100.00%\n",
      "Normalizing text 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Normalize the train data.\n",
    "normalize(train_data)\n",
    "\n",
    "# Normalize the test data.\n",
    "normalize(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTrain Data Example:\u001b[0m\n",
      "Undiacritized: sinif havuz ve acik deniz calismalariyla tum dunyada gecerli basarili bir standart olusturmustur \n",
      "Diacritized: sınıf havuz ve açık deniz çalışmalarıyla tüm dünyada geçerli başarılı bir standart oluşturmuştur \n",
      "\n",
      "\u001b[1mTest Data Example:\u001b[0m\n",
      "Undiacritized:  tr ekonomi ve politika haberleri turkiye nin en cesur gazetesi radikal de uye ol\n",
      "Diacritized: None\n"
     ]
    }
   ],
   "source": [
    "print_example([train_data.get(0, 'und'), train_data.get(0, 'd'), test_data.get(0, 'und'), test_data.get(0, 'd')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, tokenize the text by splitting it into words. We will be using Spacy for tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing... 100.00%\n",
      "Tokenizing... 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the train data.\n",
    "tokenize(train_data)\n",
    "\n",
    "# Tokenize the test data.\n",
    "tokenize(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTrain Data Example:\u001b[0m\n",
      "Undiacritized: ['sinif', 'havuz', 've', 'acik', 'deniz', 'calismalariyla', 'tum', 'dunyada', 'gecerli', 'basarili', 'bir', 'standart', 'olusturmustur']\n",
      "Diacritized: ['sınıf', 'havuz', 've', 'açık', 'deniz', 'çalışmalarıyla', 'tüm', 'dünyada', 'geçerli', 'başarılı', 'bir', 'standart', 'oluşturmuştur']\n",
      "\n",
      "\u001b[1mTest Data Example:\u001b[0m\n",
      "Undiacritized: ['tr', 'ekonomi', 've', 'politika', 'haberleri', 'turkiye', 'nin', 'en', 'cesur', 'gazetesi', 'radikal', 'de', 'uye', 'ol']\n",
      "Diacritized: None\n"
     ]
    }
   ],
   "source": [
    "print_example([train_data.get(0, 'und'), train_data.get(0, 'd'), test_data.get(0, 'und'), test_data.get(0, 'd')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build word to index and index to word mappings.\n",
    "vocab = train_data.build_vocab(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, apply padding to fit all the sentences into one length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Length: 118\n"
     ]
    }
   ],
   "source": [
    "# Pad the datasets.\n",
    "max_len = train_data.pad()\n",
    "test_data.pad(max_len)\n",
    "\n",
    "# Print the maximum length.\n",
    "print(f'Maximum Length: {max_len}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the vocabulary, convert the words into indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the strings to vocabular integers (mappings).\n",
    "train_data.to_indices()\n",
    "test_data.to_indices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the train data.\n",
    "train_data.save_data('data/train_data.pkl')\n",
    "\n",
    "# Save the test data.\n",
    "test_data.save_data('data/test_data.pkl')\n",
    "\n",
    "# Save the vocab.\n",
    "train_data.save_vocab('data/vocab.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTrain Data Example:\u001b[0m\n",
      "Undiacritized: <sos> sinif havuz ve acik deniz calismalariyla tum dunyada gecerli basarili bir standart olusturmustur <eos> <pad> (105)...\n",
      "Diacritized: <sos> sınıf havuz ve açık deniz çalışmalarıyla tüm dünyada geçerli başarılı bir standart oluşturmuştur <eos> <pad> (105)...\n",
      "\n",
      "\u001b[1mTest Data Example:\u001b[0m\n",
      "Undiacritized: <sos> tr ekonomi ve politika haberleri turkiye nin en cesur gazetesi radikal de uye ol <eos> <pad> (104)...\n",
      "Diacritized: None\n"
     ]
    }
   ],
   "source": [
    "print_example([\n",
    "    untokenize(train_data, 0, 'und', detailed=True),\n",
    "    untokenize(train_data, 0, 'd', detailed=True),\n",
    "    untokenize(test_data, 0, 'und', detailed=True),\n",
    "    None\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
