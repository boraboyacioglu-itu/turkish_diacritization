{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turkish Diacritisation | YZV 405E NLP Term Project\n",
    "\n",
    "Author: Bora Boyacıoğlu\n",
    "\n",
    "Student ID: 150200310\n",
    "\n",
    "## Step 4: Rule Based Algorithm\n",
    "\n",
    "In this notebook, my aim is to develop a rule based algorithm to find the words that are actually in need to be replaced. This will massively improve the accuracy, as most of the words do not require any changes at all. And most of the one do require, only have one correspondance. The logic here is like the following:\n",
    "\n",
    "1. Unicode Turkish word does not change. *(eg. bilgisayar)*\n",
    "2. If a word only has one correspondance, check for it in the vocabulary and replace it with that one. *(eg. sinif $\\rightarrow$ sınıf)*\n",
    "3. In case a word has more than one acronyms, list every possible combination. *(eg. aci $\\rightarrow$ {acı, açı})*\n",
    "    * Create a possibility pool for each acronym.\n",
    "    * 3.1. Give the sentence to the model. If the prediction sentence contains any acronyms of that word, use it.\n",
    "    * 3.2. If not, replace the word with the most probable combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from IPython.display import Markdown as md\n",
    "from unidecode import unidecode\n",
    "\n",
    "from dataset import DiacritizationDataset\n",
    "from utils.main_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Vocabulary\n",
    "\n",
    "I will be using two vocabularies:\n",
    "1. Turkish Dictionary **[1]**\n",
    "4. Train Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Turkish Dictionary\n",
    "\n",
    "The file seems to be a broken JSON format. So, I needed to manually find the word by splitting each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Turkish character transformation.\n",
    "tr_maps = str.maketrans({'â': 'a', 'î': 'i', 'û': 'ü'})\n",
    "\n",
    "# Define the normalization and tokenization function.\n",
    "nt = lambda x: tokenize_text(normalize_str(x.translate(tr_maps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset.\n",
    "with open('data/ext/gts.json') as f:\n",
    "    data = f.read().splitlines()\n",
    "    words1 = []\n",
    "    \n",
    "    for line in data:\n",
    "        # Skip lines which may cause problems, if any.\n",
    "        if '\"madde\":\"' not in line:\n",
    "            continue\n",
    "        \n",
    "        # Get the word.\n",
    "        madde = line.split('\"madde\":\"')[1].split('\"')[0]\n",
    "        \n",
    "        # Normalize and tokenize the word.\n",
    "        madde = nt(madde)\n",
    "                \n",
    "        words1.extend(madde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 148270\n"
     ]
    }
   ],
   "source": [
    "print(\"Length:\", len(words1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Train Data\n",
    "\n",
    "This load will be different from the first part, as I will only do the early preprocessing steps (filtering, normalising, and tokenising). The vocabulary creation process is different here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing text 100.00%\n",
      "Tokenizing... 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset.\n",
    "train_data = DiacritizationDataset('data/train.csv', type='train')\n",
    "\n",
    "# Normalize the train data.\n",
    "normalize(train_data)\n",
    "\n",
    "# Tokenize the train data.\n",
    "tokenize(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the words.\n",
    "words2 = []\n",
    "\n",
    "for sent in train_data.diacritized:\n",
    "    words2.extend(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 689839\n"
     ]
    }
   ],
   "source": [
    "print(\"Length:\", len(words2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine the Vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique words.\n",
    "words = list(set(words1 + words2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Length: 150119\n"
     ]
    }
   ],
   "source": [
    "j1 = len(words)\n",
    "print(\"1. Length:\", j1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the acronyms.\n",
    "acronyms = {}\n",
    "\n",
    "for word in words:\n",
    "    undiacritized = unidecode(word)\n",
    "    \n",
    "    # If the word only contains ASCII characters, skip it.\n",
    "    if undiacritized == word:\n",
    "        continue\n",
    "    \n",
    "    # Add the undiacritized word to the acronyms.\n",
    "    if undiacritized not in acronyms:\n",
    "        acronyms[undiacritized] = [word]\n",
    "    else:\n",
    "        acronyms[undiacritized].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Acronyms: 85075\n"
     ]
    }
   ],
   "source": [
    "j2 = len(acronyms)\n",
    "print(\"2. Acronyms:\", j2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the words which have more than one acronym.\n",
    "plural = 0\n",
    "for undiacritized, words in list(acronyms.items()):\n",
    "    if len(words) > 1:\n",
    "        plural += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Words with more than one acronym: 833\n"
     ]
    }
   ],
   "source": [
    "j3 = plural\n",
    "print(\"3. Words with more than one acronym:\", j3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total acronyms.\n",
    "total_acronyms = 0\n",
    "total_plural = 0\n",
    "for words in acronyms.values():\n",
    "    total_acronyms += len(words)\n",
    "    if len(words) > 1:\n",
    "        total_plural += len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Total plural acronym count: 1702\n"
     ]
    }
   ],
   "source": [
    "j4 = total_plural\n",
    "print(\"4. Total plural acronym count:\", j4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, out of 150119 unique words **(1)** in our vocabulary, only 85075 **(2)** have non-ASCII forms. This is only the $56.67\\%$ of the unique words. And out of 85075, only 833 **(3)** share different acronym forms with each other. This is $0.98\\%$ of the non-ASCII ones. And after considering the total number of acronyms left, we have 1702 words **(4)**, out of 150119 in total. To conclude, only $1.13\\%$ of the words require a prediction to be changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting the Probabilities\n",
    "\n",
    "Now, the goal is to count the occurrences of each acronym in the Train Data. The reason I am only taking the train data to account is, I need the chance of a word occuring. The first data is, by its name, a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting... 100.00%\r"
     ]
    }
   ],
   "source": [
    "# Count the acronyms.\n",
    "probs = {}\n",
    "index = 0\n",
    "for acronym, words in acronyms.items():\n",
    "    counts = {}\n",
    "    \n",
    "    if len(words) == 1:\n",
    "        print(f'Counting... {100 * (index + 1) / total_acronyms:.2f}%', end='\\r')\n",
    "        probs[acronym] = {words[0]: 1}\n",
    "        index += 1\n",
    "        continue\n",
    "    \n",
    "    for word in words:\n",
    "        print(f'Counting... {100 * (index + 1) / total_acronyms:.2f}%', end='\\r')\n",
    "        counts[word] = words2.count(word)\n",
    "        index += 1\n",
    "        \n",
    "    probs[acronym] = counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the Probabilities\n",
    "\n",
    "Lastly, save the counted probabilities into a nested JSON dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the probabilities.\n",
    "with open('data/comb/probs.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(probs, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "**[1] Turkish Dictionary:**\n",
    "\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2021 Kemal Ogun Isik\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "GitHub @ guncel-turkce-sozluk (https://github.com/ogun/guncel-turkce-sozluk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
