{"cells":[{"cell_type":"markdown","metadata":{"id":"7LuTABulY5Vc"},"source":["# Turkish Diacritisation | YZV 405E NLP Term Project\n","\n","Author: Bora Boyacıoğlu\n","\n","Student ID: 150200310\n","\n","## Step 2: Training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7998,"status":"ok","timestamp":1714990707498,"user":{"displayName":"Bora Boyacıoğlu","userId":"16547303742894524125"},"user_tz":-180},"id":"-1_vChv-Y5Vh","outputId":"64c1b9a2-e7db-470a-c178-97f2666cc5f3"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/235.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/235.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m225.3/235.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install unidecode --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U_hB_02aY5Vi"},"outputs":[],"source":["import os\n","import sys\n","import time\n","from datetime import datetime as dt\n","\n","import numpy as np\n","import pickle as pkl\n","\n","import torch\n","from torch import nn\n","from torch import optim\n","from torch.utils.data import DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5DIuBMapY5Vj"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"TOIfE9SNY5Vj"},"source":["### Reloading the Processed Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NWlseU6zIwtH"},"outputs":[],"source":["path = ''"]},{"cell_type":"markdown","metadata":{"id":"5MkclWqFns6G"},"source":["Mount the drive for Google Colab. <font color='red'>Do not run this for local use.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24069,"status":"ok","timestamp":1714990734849,"user":{"displayName":"Bora Boyacıoğlu","userId":"16547303742894524125"},"user_tz":-180},"id":"tIrifLvgI2JO","outputId":"a36bd62f-420d-402c-d9bd-77081651f3a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# Mound the Drive.\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Delete the sample_data folder because I don't like unnecessary things.\n","!rm -rf sample_data\n","\n","# Update the data folder.\n","path = '/content/drive/MyDrive/Share/NLP/'\n","\n","# Append the data folder path to system.\n","sys.path.append(path)"]},{"cell_type":"markdown","metadata":{"id":"Dg5phW6Zny3o"},"source":["Import local libraries and classes. Open data and vocab files."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w5f2Y81gY5Vk"},"outputs":[],"source":["# Import local files.\n","from utils.main_utils import *\n","from utils.model import Encoder, Decoder, Seq2Seq\n","\n","# Load the train dataset.\n","with open(path + 'data/train_data.pkl', 'rb') as f:\n","    train_data = pkl.load(f)\n","\n","# Load the test dataset.\n","with open(path + 'data/test_data.pkl', 'rb') as f:\n","    test_data = pkl.load(f)\n","\n","# Load the vocab.\n","with open(path + 'data/vocab.pkl', 'rb') as f:\n","    vocab = pkl.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vL9BHr1Z7Iuv"},"outputs":[],"source":["train_und = np.array(train_data.undiacritized)\n","train_d = np.array(train_data.diacritized)\n","\n","test_und = np.array(test_data.undiacritized)\n","test_d = np.array(test_data.diacritized)\n","\n","vocab_size = len(vocab['w2i'])\n","max_len = train_und.shape[1]"]},{"cell_type":"markdown","metadata":{"id":"eBxi_CyhY5Vk"},"source":["### Initialising or Reloading the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0qomMl6_mN-Z"},"outputs":[],"source":["emb_dim = 64\n","hid_dim = 256\n","n_layers = 2\n","dropout = 0.5\n","batch_size = 18\n","clip = 1\n","early_stop = 3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-F4QrdVWdk-0"},"outputs":[],"source":["new = False\n","loaded_loss = None\n","loaded_epoch = None\n","loaded_timestamp = None"]},{"cell_type":"markdown","metadata":{"id":"z6aXG9AIdSWI"},"source":["Initialise the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ep54KOutY5Vm"},"outputs":[],"source":["# The device.\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Training parameters.\n","params = {\n","    'emb_dim': emb_dim,\n","    'hid_dim': hid_dim,\n","    'n_layers': n_layers,\n","    'dropout': dropout\n","}\n","\n","# Define the models.\n","encoder = Encoder(input_dim=vocab_size, **params)\n","decoder = Decoder(output_dim=vocab_size - 1, **params)\n","model = Seq2Seq(encoder, decoder, device).to(device)\n","\n","# Define the optimiser and the loss function.\n","optimizer = optim.Adam(model.parameters())\n","criterion = nn.CrossEntropyLoss(ignore_index=vocab['w2i'][\"<pad>\"])"]},{"cell_type":"markdown","metadata":{"id":"bKBuhdDcdU7E"},"source":["Or, load the previous model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OgJu5hDLlIg1"},"outputs":[],"source":["if not new:\n","    # Set the names.\n","    timestamp = '2024-05-05_18-42-23'\n","    name = 'e22-l5.27-p64_256_2_0.5_18'\n","\n","    # Load the saved checkpoint.\n","    checkpoint = torch.load(f'{path}models/{timestamp}/{name}.pth', map_location=device)\n","\n","    # Load the model and optimizer.\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","    # Set the loaded variables.\n","    loaded_loss = checkpoint['loss']\n","    loaded_epoch = checkpoint['epoch']\n","    loaded_timestamp = timestamp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BpLPnPY3Y5Vm"},"outputs":[],"source":["params['batch_size'] = batch_size\n","\n","# Define the data loader.\n","loader = DataLoader(train_data, batch_size=params['batch_size'], shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"Srgkdw_0ne03"},"source":["Define the train loop."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VW0e6fXqnN4R"},"outputs":[],"source":["def train(model, loader, optimizer, criterion, clip, device, verbose=False) -> float:\n","    \"\"\" Training function.\n","    Args:\n","        model (Seq2Seq): The model.\n","        loader (DataLoader): The data loader.\n","        optimizer (optim.Adam): The optimiser.\n","        criterion (nn.CrossEntropyLoss): The loss function.\n","        clip (float): The gradient clipping value.\n","        device (torch.device): The device.\n","\n","    Returns:\n","        epoch_loss (float): The epoch loss.\n","    \"\"\"\n","    time_init = time.time()\n","    skipped = 0\n","\n","    model.train()\n","    epoch_loss = 0\n","\n","    for i, (und, d) in enumerate(loader):\n","        # Move the data to the device.\n","        und, d = und.to(device), d.to(device)\n","        optimizer.zero_grad()\n","\n","        # Run the model.\n","        try:\n","            output = model(und, d)\n","\n","            # Reshape.\n","            output_dim = output.shape[-1]\n","            output = output[1:].view(-1, output_dim)\n","            d = d[1:].view(-1)\n","\n","            # Calculate the loss.\n","            loss = criterion(output, d)\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","        except RuntimeError as e:\n","            skipped += 1\n","\n","        # Verbose the batch.\n","        verbose_batch(i, time_init, len(loader), skipped, epoch_loss) if verbose else None\n","\n","    return epoch_loss / (len(loader) - skipped)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1714990764607,"user":{"displayName":"Bora Boyacıoğlu","userId":"16547303742894524125"},"user_tz":-180},"id":"jPHCt3KbntpT","outputId":"4d354f4d-eacb-4893-9a43-9e270a29e66f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Timestamp: 2024-05-05_18-42-23\n"]}],"source":["# Define the train loop values.\n","timestamp = loaded_timestamp or dt.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n","!mkdir -p {path}models/{timestamp}\n","\n","print(f\"Timestamp: {timestamp}\")\n","\n","num_epochs = 40\n","epoch = loaded_epoch or 0\n","not_improved = 0\n","\n","# Check if there is a loaded loss.\n","best_loss = loaded_loss or float('inf')\n","prev_save = None"]},{"cell_type":"markdown","metadata":{"id":"j6lFcNcOnnuH"},"source":["Start training."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yzJC7Bg7QBel","outputId":"3b4c61ee-4c8a-435a-fa1d-6e508f71a637"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[92mTraining started from epoch 23 for 40 epochs with parameters emb_dim=64, hid_dim=256, n_layers=2, dropout=0.5, batch_size=18.\u001b[0m\n","\n","Batch 3202/3202 [====================] (100.00%), Epoch Loss: 5.2030, Skipped: 0, Elapsed: 31:34, Speed: 0.59s/batch, Remaining: 00:00\n","\u001b[93mEpoch: 23/40, Train Loss: 5.2030\u001b[0m [Saved as 'e23-l5.20-p64_256_2_0.5_18.pth']\n","\n","Batch 3202/3202 [====================] (100.00%), Epoch Loss: 5.1285, Skipped: 0, Elapsed: 32:01, Speed: 0.60s/batch, Remaining: 00:00\n","\u001b[93mEpoch: 24/40, Train Loss: 5.1285\u001b[0m [Saved as 'e24-l5.13-p64_256_2_0.5_18.pth']\n","\n","Batch 3202/3202 [====================] (100.00%), Epoch Loss: 5.0610, Skipped: 0, Elapsed: 31:30, Speed: 0.59s/batch, Remaining: 00:00\n","\u001b[93mEpoch: 25/40, Train Loss: 5.0610\u001b[0m [Saved as 'e25-l5.06-p64_256_2_0.5_18.pth']\n","\n","Batch 3202/3202 [====================] (100.00%), Epoch Loss: 5.0005, Skipped: 0, Elapsed: 30:58, Speed: 0.58s/batch, Remaining: 00:00\n","\u001b[93mEpoch: 26/40, Train Loss: 5.0005\u001b[0m [Saved as 'e26-l5.00-p64_256_2_0.5_18.pth']\n","\n","Batch 3202/3202 [====================] (100.00%), Epoch Loss: 4.9540, Skipped: 0, Elapsed: 30:37, Speed: 0.57s/batch, Remaining: 00:00\n","\u001b[93mEpoch: 27/40, Train Loss: 4.9540\u001b[0m [Saved as 'e27-l4.95-p64_256_2_0.5_18.pth']\n","\n","Batch 1253/3202 [=======             ] (39.13%), Epoch Loss: 4.8847, Skipped: 0, Elapsed: 11:59, Speed: 0.57s/batch, Remaining: 18:39"]}],"source":["print(\"\\033[92m\"\n","    f\"Training started\",\n","    str(f\"from epoch {epoch + 1} \" if epoch > 0 else \"\") +\n","    f\"for {num_epochs} epoch\" + ('s' if num_epochs > 1 else ''),\n","    f\"with parameters\",\n","    f\"{', '.join(f'{key}={value}' for key, value in params.items())}\",\n","    sep=\" \", end=\".\\033[0m\\n\\n\"\n",")\n","\n","verbose = True\n","del_prev = True\n","\n","if epoch == num_epochs:\n","    num_epochs += 1\n","\n","while epoch < num_epochs:\n","    try:\n","        # Train the model.\n","        train_loss = train(model, loader, optimizer, criterion, clip, device, verbose)\n","\n","        # Increment the epoch.\n","        epoch += 1\n","\n","        # Verbose the epoch.\n","        print(f\"\\n\\033[93mEpoch: {epoch}/{num_epochs}, Train Loss: {train_loss:.4f}\\033[0m\", end=\"\")\n","\n","        # Check if the loss decreased.\n","        if train_loss > best_loss:\n","            print(\"\\n\")\n","\n","            # Check for the early stop criteria.\n","            not_improved += 1\n","            if not_improved > early_stop:\n","                print(f\"\\n\\033[92mEarly stopping at epoch {epoch}. The best loss is {best_loss}.\\033[0m\")\n","                break\n","\n","            continue\n","\n","        # Update the loss.\n","        best_loss = train_loss\n","\n","        # Determine the save addresses.\n","        file_name = f\"e{epoch}-l{train_loss:.2f}-p{'_'.join(str(param) for param in params.values())}.pth\"\n","        save_name = f\"{path}models/{timestamp}/{file_name}\"\n","\n","        # Save the model.\n","        torch.save({\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': best_loss,\n","            'epoch': epoch\n","        }, save_name)\n","\n","\n","        # Remove the previous save.\n","        if prev_save and del_prev:\n","            os.remove(prev_save)\n","\n","        prev_save = save_name\n","\n","        print(f\" [Saved as '{file_name}']\\n\")\n","    except KeyboardInterrupt:\n","        print(f\"\\n\\033[91mTraining interrupted at epoch {epoch + 1}.\\033[0m\")\n","        break"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}
