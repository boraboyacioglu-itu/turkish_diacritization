{"cells":[{"cell_type":"markdown","metadata":{"id":"7LuTABulY5Vc"},"source":["# Turkish Diacritisation | YZV 405E NLP Term Project\n","\n","Author: Bora Boyacıoğlu\n","\n","Student ID: 150200310\n","\n","## Step 2: Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-1_vChv-Y5Vh","executionInfo":{"status":"ok","timestamp":1715026787455,"user_tz":-180,"elapsed":3020,"user":{"displayName":"Bora Boyacıoğlu","userId":"16547303742894524125"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"678d8487-3d86-4008-93c1-86ed01a46dee"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m225.3/235.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install unidecode --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U_hB_02aY5Vi"},"outputs":[],"source":["import os\n","import sys\n","import time\n","from datetime import datetime as dt\n","\n","import numpy as np\n","import pickle as pkl\n","\n","import torch\n","from torch import nn\n","from torch import optim\n","from torch.utils.data import DataLoader\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5DIuBMapY5Vj"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"TOIfE9SNY5Vj"},"source":["### Reloading the Processed Data"]},{"cell_type":"code","source":["path = ''"],"metadata":{"id":"NWlseU6zIwtH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Mount the drive for Google Colab. <font color='red'>Do not run this for local use.</font>"],"metadata":{"id":"5MkclWqFns6G"}},{"cell_type":"code","source":["# Mound the Drive.\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Delete the sample_data folder because I don't like unnecessary things.\n","!rm -rf sample_data\n","\n","# Update the data folder.\n","path = '/content/drive/MyDrive/Share/NLP/'\n","\n","# Append the data folder path to system.\n","sys.path.append(path)"],"metadata":{"id":"tIrifLvgI2JO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715026840522,"user_tz":-180,"elapsed":50224,"user":{"displayName":"Bora Boyacıoğlu","userId":"16547303742894524125"}},"outputId":"a8187ac6-ac5e-4cd8-e819-5777ec18c598"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["Import local libraries and classes. Open data and vocab files."],"metadata":{"id":"Dg5phW6Zny3o"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"w5f2Y81gY5Vk"},"outputs":[],"source":["# Import local files.\n","from utils.main_utils import *\n","from utils.model import Encoder, Decoder, Seq2Seq\n","\n","# Load the train dataset.\n","with open(path + 'data/train_data.pkl', 'rb') as f:\n","    train_data = pkl.load(f)\n","\n","# Load the test dataset.\n","with open(path + 'data/test_data.pkl', 'rb') as f:\n","    test_data = pkl.load(f)\n","\n","# Load the vocab.\n","with open(path + 'data/vocab.pkl', 'rb') as f:\n","    vocab = pkl.load(f)"]},{"cell_type":"code","source":["train_und = np.array(train_data.undiacritized)\n","train_d = np.array(train_data.diacritized)\n","\n","test_und = np.array(test_data.undiacritized)\n","test_d = np.array(test_data.diacritized)\n","\n","vocab_size = len(vocab['w2i'])\n","max_len = train_und.shape[1]"],"metadata":{"id":"vL9BHr1Z7Iuv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eBxi_CyhY5Vk"},"source":["### Initialising or Reloading the Model"]},{"cell_type":"code","source":["emb_dim = 64\n","hid_dim = 256\n","n_layers = 2\n","dropout = 0.5\n","batch_size = 18\n","clip = 1\n","early_stop = 3"],"metadata":{"id":"0qomMl6_mN-Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new = False\n","loaded_loss = None\n","loaded_epoch = None\n","loaded_timestamp = None"],"metadata":{"id":"-F4QrdVWdk-0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Initialise the model."],"metadata":{"id":"z6aXG9AIdSWI"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ep54KOutY5Vm"},"outputs":[],"source":["# The device.\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Training parameters.\n","params = {\n","    'emb_dim': emb_dim,\n","    'hid_dim': hid_dim,\n","    'n_layers': n_layers,\n","    'dropout': dropout\n","}\n","\n","# Define the models.\n","encoder = Encoder(input_dim=vocab_size, **params)\n","decoder = Decoder(output_dim=vocab_size - 1, **params)\n","model = Seq2Seq(encoder, decoder, device).to(device)\n","\n","# Define the optimiser and the loss function.\n","optimizer = optim.Adam(model.parameters())\n","criterion = nn.CrossEntropyLoss(ignore_index=vocab['w2i'][\"<pad>\"])"]},{"cell_type":"markdown","source":["Or, load the previous model."],"metadata":{"id":"bKBuhdDcdU7E"}},{"cell_type":"code","source":["if not new:\n","    # Set the names.\n","    timestamp = '2024-05-05_18-42-23'\n","    name = 'e37-l4.42-p64_256_2_0.5_18'\n","\n","    # Load the saved checkpoint.\n","    checkpoint = torch.load(f'{path}models/{timestamp}/{name}.pth', map_location=device)\n","\n","    # Load the model and optimizer.\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","    # Set the loaded variables.\n","    loaded_loss = checkpoint['loss']\n","    loaded_epoch = checkpoint['epoch']\n","    loaded_timestamp = timestamp"],"metadata":{"id":"OgJu5hDLlIg1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BpLPnPY3Y5Vm"},"outputs":[],"source":["params['batch_size'] = batch_size\n","\n","# Define the data loader.\n","loader = DataLoader(train_data, batch_size=params['batch_size'], shuffle=True)"]},{"cell_type":"markdown","source":["Define the train loop."],"metadata":{"id":"Srgkdw_0ne03"}},{"cell_type":"code","source":["def train(model, loader, optimizer, criterion, clip, device, verbose=False) -> float:\n","    \"\"\" Training function.\n","    Args:\n","        model (Seq2Seq): The model.\n","        loader (DataLoader): The data loader.\n","        optimizer (optim.Adam): The optimiser.\n","        criterion (nn.CrossEntropyLoss): The loss function.\n","        clip (float): The gradient clipping value.\n","        device (torch.device): The device.\n","\n","    Returns:\n","        epoch_loss (float): The epoch loss.\n","    \"\"\"\n","    time_init = time.time()\n","    skipped = 0\n","\n","    model.train()\n","    epoch_loss = 0\n","\n","    for i, (und, d) in enumerate(loader):\n","        # Move the data to the device.\n","        und, d = und.to(device), d.to(device)\n","        optimizer.zero_grad()\n","\n","        # Run the model.\n","        try:\n","            output = model(und, d)\n","\n","            # Reshape.\n","            output_dim = output.shape[-1]\n","            output = output[1:].view(-1, output_dim)\n","            d = d[1:].view(-1)\n","\n","            # Calculate the loss.\n","            loss = criterion(output, d)\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","        except RuntimeError as e:\n","            skipped += 1\n","\n","        # Verbose the batch.\n","        verbose_batch(i, time_init, len(loader), skipped, epoch_loss) if verbose else None\n","\n","    return epoch_loss / (len(loader) - skipped)"],"metadata":{"id":"VW0e6fXqnN4R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the train loop values.\n","timestamp = loaded_timestamp or dt.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n","!mkdir -p {path}models/{timestamp}\n","\n","print(f\"Timestamp: {timestamp}\")\n","\n","num_epochs = 40\n","epoch = loaded_epoch or 0\n","not_improved = 0\n","\n","# Check if there is a loaded loss.\n","best_loss = loaded_loss or float('inf')\n","prev_save = None"],"metadata":{"id":"jPHCt3KbntpT","executionInfo":{"status":"ok","timestamp":1715026864787,"user_tz":-180,"elapsed":317,"user":{"displayName":"Bora Boyacıoğlu","userId":"16547303742894524125"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"99b767b1-ed7d-4e79-c6d2-6fe3299a6f52"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Timestamp: 2024-05-05_18-42-23\n"]}]},{"cell_type":"markdown","source":["Start training."],"metadata":{"id":"j6lFcNcOnnuH"}},{"cell_type":"code","source":["print(\"\\033[92m\"\n","    f\"Training started\",\n","    str(f\"from epoch {epoch + 1} \" if epoch > 0 else \"\") +\n","    f\"for {num_epochs} epoch\" + ('s' if num_epochs > 1 else ''),\n","    f\"with parameters\",\n","    f\"{', '.join(f'{key}={value}' for key, value in params.items())}\",\n","    sep=\" \", end=\".\\033[0m\\n\\n\"\n",")\n","\n","verbose = True\n","del_prev = True\n","\n","if epoch == num_epochs:\n","    num_epochs += 1\n","\n","while epoch < num_epochs:\n","    try:\n","        # Train the model.\n","        train_loss = train(model, loader, optimizer, criterion, clip, device, verbose)\n","\n","        # Increment the epoch.\n","        epoch += 1\n","\n","        # Verbose the epoch.\n","        print(f\"\\n\\033[93mEpoch: {epoch}/{num_epochs}, Train Loss: {train_loss:.4f}\\033[0m\", end=\"\")\n","\n","        # Check if the loss decreased.\n","        if train_loss > best_loss:\n","            print(\"\\n\")\n","\n","            # Check for the early stop criteria.\n","            not_improved += 1\n","            if not_improved > early_stop:\n","                print(f\"\\n\\033[92mEarly stopping at epoch {epoch}. The best loss is {best_loss}.\\033[0m\")\n","                break\n","\n","            continue\n","\n","        # Update the loss.\n","        best_loss = train_loss\n","\n","        # Determine the save addresses.\n","        file_name = f\"e{epoch}-l{train_loss:.2f}-p{'_'.join(str(param) for param in params.values())}.pth\"\n","        save_name = f\"{path}models/{timestamp}/{file_name}\"\n","\n","        # Save the model.\n","        torch.save({\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': best_loss,\n","            'epoch': epoch\n","        }, save_name)\n","\n","        # Remove the previous save.\n","        if prev_save and del_prev:\n","            os.remove(prev_save)\n","        prev_save = save_name\n","\n","        print(f\" [Saved as '{file_name}']\\n\")\n","    except KeyboardInterrupt:\n","        print(f\"\\n\\033[91mTraining interrupted at epoch {epoch + 1}.\\033[0m\")\n","        break\n","\n","print(f\"\\n\\033[92mTraining completed. The best loss is {best_loss}.\\033[0m\")"],"metadata":{"id":"yzJC7Bg7QBel","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c55e00c5-f6a7-4e76-cf98-2d364578afdb","executionInfo":{"status":"ok","timestamp":1715068822513,"user_tz":-180,"elapsed":160920,"user":{"displayName":"Bora Boyacıoğlu","userId":"16547303742894524125"}}},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[92mTraining started from epoch 41 for 60 epochs with parameters emb_dim=64, hid_dim=256, n_layers=2, dropout=0.5, batch_size=18.\u001b[0m\n","\n","Batch 3202/3202 [====================] (100.00%), Epoch Loss: 4.2284, Skipped: 0, Elapsed: 45:11, Speed: 0.85s/batch, Remaining: 00:00\n","\u001b[93mEpoch: 41/60, Train Loss: 4.2284\u001b[0m [Saved as 'e41-l4.23-p64_256_2_0.5_18.pth']\n","\n","Batch 3202/3202 [====================] (100.00%), Epoch Loss: 4.1811, Skipped: 0, Elapsed: 45:21, Speed: 0.85s/batch, Remaining: 00:00\n","\u001b[93mEpoch: 42/60, Train Loss: 4.1811\u001b[0m [Saved as 'e42-l4.18-p64_256_2_0.5_18.pth']\n","\n","Batch 3202/3202 [====================] (100.00%), Epoch Loss: 4.1417, Skipped: 0, Elapsed: 45:19, Speed: 0.85s/batch, Remaining: 00:00\n","\u001b[93mEpoch: 43/60, Train Loss: 4.1417\u001b[0m [Saved as 'e43-l4.14-p64_256_2_0.5_18.pth']\n","\n","Batch 3202/3202 [====================] (100.00%), Epoch Loss: 4.0989, Skipped: 0, Elapsed: 45:25, Speed: 0.85s/batch, Remaining: 00:00\n","\u001b[93mEpoch: 44/60, Train Loss: 4.0989\u001b[0m [Saved as 'e44-l4.10-p64_256_2_0.5_18.pth']\n","\n","Batch 3202/3202 [====================] (100.00%), Epoch Loss: 4.0534, Skipped: 0, Elapsed: 45:25, Speed: 0.85s/batch, Remaining: 00:00\n","\u001b[93mEpoch: 45/60, Train Loss: 4.0534\u001b[0m [Saved as 'e45-l4.05-p64_256_2_0.5_18.pth']\n","\n","Batch 3202/3202 [====================] (100.00%), Epoch Loss: 4.0230, Skipped: 0, Elapsed: 45:26, Speed: 0.85s/batch, Remaining: 00:00\n","\u001b[93mEpoch: 46/60, Train Loss: 4.0230\u001b[0m [Saved as 'e46-l4.02-p64_256_2_0.5_18.pth']\n","\n","Batch 3202/3202 [====================] (100.00%), Epoch Loss: 3.9822, Skipped: 0, Elapsed: 45:27, Speed: 0.85s/batch, Remaining: 00:00\n","\u001b[93mEpoch: 47/60, Train Loss: 3.9822\u001b[0m [Saved as 'e47-l3.98-p64_256_2_0.5_18.pth']\n","\n","Batch 3202/3202 [====================] (100.00%), Epoch Loss: 3.9522, Skipped: 0, Elapsed: 45:29, Speed: 0.85s/batch, Remaining: 00:00\n","\u001b[93mEpoch: 48/60, Train Loss: 3.9522\u001b[0m [Saved as 'e48-l3.95-p64_256_2_0.5_18.pth']\n","\n","Batch 3202/3202 [====================] (100.00%), Epoch Loss: 3.9125, Skipped: 0, Elapsed: 45:30, Speed: 0.85s/batch, Remaining: 00:00\n","\u001b[93mEpoch: 49/60, Train Loss: 3.9125\u001b[0m [Saved as 'e49-l3.91-p64_256_2_0.5_18.pth']\n","\n","Batch 3202/3202 [====================] (100.00%), Epoch Loss: 3.8874, Skipped: 0, Elapsed: 45:32, Speed: 0.85s/batch, Remaining: 00:00\n","\u001b[93mEpoch: 50/60, Train Loss: 3.8874\u001b[0m [Saved as 'e50-l3.89-p64_256_2_0.5_18.pth']\n","\n","Batch 855/3202 [=====               ] (26.70%), Epoch Loss: 3.7863, Skipped: 0, Elapsed: 12:11, Speed: 0.86s/batch, Remaining: 33:27\n","\u001b[91mTraining interrupted at epoch 51.\u001b[0m\n","\n","\u001b[92mTraining completed. The best loss is 3.887409987410927.\u001b[0m\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}