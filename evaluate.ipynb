{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turkish Diacritisation | YZV 405E NLP Term Project\n",
    "\n",
    "Author: Bora Boyacıoğlu\n",
    "\n",
    "Student ID: 150200310\n",
    "\n",
    "## Step 3: Evaluating\n",
    "\n",
    "**Note:** This notebook is not designed for Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install unidecode --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import pickle as pkl\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reloading the Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mount the drive for Google Colab. <font color='red'>Do not run this for local use.</font>\n",
    "\n",
    "`Command + Shift + 7` to comment out or uncomment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mound the Drive.\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# # Delete the sample_data folder because I don't like unnecessary things.\n",
    "# !rm -rf sample_data\n",
    "\n",
    "# # Update the data folder.\n",
    "# path = '/content/drive/MyDrive/Share/NLP/'\n",
    "\n",
    "# # Append the data folder path to system.\n",
    "# sys.path.append(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import local libraries and classes. Open data and vocab files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model import Encoder, Decoder, Seq2Seq\n",
    "from utils.utils import *\n",
    "\n",
    "# Load the test data.\n",
    "with open(path + 'data/test_data.pkl', 'rb') as f:\n",
    "    test_data = pkl.load(f)\n",
    "\n",
    "# Load the vocabulary.\n",
    "with open(path + 'data/vocab.pkl', 'rb') as f:\n",
    "    vocab = pkl.load(f)\n",
    "\n",
    "length = len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reloading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(145437, 64)\n",
       "    (rnn): LSTM(64, 256, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(145436, 64)\n",
       "    (rnn): LSTM(64, 256, num_layers=2, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=256, out_features=145436, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the device.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the parameters (same as the ones used for training).\n",
    "params = {\n",
    "    'emb_dim': 64,\n",
    "    'hid_dim': 256,\n",
    "    'n_layers': 2,\n",
    "    'dropout': 0.5\n",
    "}\n",
    "\n",
    "# Initialize the model.\n",
    "encoder = Encoder(input_dim=len(vocab['w2i']), **params)\n",
    "decoder = Decoder(output_dim=len(vocab['w2i']) - 1, **params)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "# Load the model's state_dict.\n",
    "checkpoint = torch.load(path + 'models/e50-l3.89-p64_256_2_0.5_18.pth', map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the prediction function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, sentence, vocab, device):\n",
    "    max_len = len(sentence)\n",
    "        \n",
    "    # Convert sentence to tensor and add a batch dimension.\n",
    "    sentence_tensor = torch.tensor(sentence, dtype=torch.long).unsqueeze(1).to(device)\n",
    "    \n",
    "    # Initialize.\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(sentence_tensor)\n",
    "\n",
    "    outputs = []\n",
    "    input = sentence_tensor[0,:]  # Start token, typically.\n",
    "    \n",
    "    # Generate sequence.\n",
    "    for _ in range(max_len):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.decoder(input, hidden, cell)\n",
    "            \n",
    "            # Get the most probable next word index.\n",
    "            top1 = output.argmax(1)\n",
    "        \n",
    "        # Check for end of sentence token.\n",
    "        if top1.item() == vocab['w2i']['<eos>']:\n",
    "            break\n",
    "        \n",
    "        # Update input for next iteration.\n",
    "        input = top1\n",
    "        outputs.append(top1.item())\n",
    "    \n",
    "    # Convert indices to words.\n",
    "    translated_sentence = [vocab['i2w'][idx] for idx in outputs]\n",
    "    return ' '.join(translated_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a random prediction from the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted (1): yazar\n",
      "Actual    (22): yazar tayfun atay bikkinlik vermis bir yesilcam klisesine yaslanma riskini basariyla asan zengin kiz fakir oglan erkeklik uzerine vurgulariyla izlenmeyi hak ediyor\n"
     ]
    }
   ],
   "source": [
    "index = random.randint(0, length - 1)\n",
    "sentence = test_data.get(index, 'und')\n",
    "actual = untokenize(test_data, index, 'und')\n",
    "\n",
    "prediction = predict(model, sentence, vocab, device)\n",
    "print(f\"Predicted ({len(prediction.split(' '))}):\", prediction)\n",
    "print(f\"Actual    ({len(actual.split(' '))}):\", actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting\n",
    "\n",
    "Here, the entire test data will be predicted using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 100.00%"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "for i in range(length):\n",
    "    # Print the progress.\n",
    "    print(f'\\rPredicting {100 * (i + 1) / length:.2f}%', end='')\n",
    "    \n",
    "    # Get the sentence.\n",
    "    sentence = test_data.get(i, 'und')\n",
    "    \n",
    "    # Make the prediction.\n",
    "    prediction = predict(model, sentence, vocab, device)\n",
    "    \n",
    "    # Append the prediction.\n",
    "    predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the predictions to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = path + 'data/comb/predictions.csv'\n",
    "max_rows = 1156\n",
    "\n",
    "with open(save_file, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['ID', 'Sentence'])\n",
    "    \n",
    "    for i, sentence in enumerate(predictions):\n",
    "        if i > max_rows:\n",
    "            break\n",
    "        \n",
    "        if not sentence:\n",
    "            sentence = ' '\n",
    "        \n",
    "        writer.writerow([i, sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Scores\n",
    "\n",
    "By comparing the results with the golden test data, get the score points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mWord Score:\u001b[0m\t2.60%\n",
      "\u001b[1mSentence Score:\u001b[0m\t0.52%\n"
     ]
    }
   ],
   "source": [
    "get_scores(predictions[:max_rows], path + 'data/test_gold.csv');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
